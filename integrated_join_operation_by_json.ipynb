{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ab808d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import dask.bag as db  # 리스트와 유사하며 큰 데이터셋을 처리\n",
    "from dask.distributed import Client  # DASK의 분산 스케줄러 설정\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6e0eb5",
   "metadata": {},
   "source": [
    "# Attend 데이터 df_attend로 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a42ed1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "attend_directory = \"D:/DATA_PREPROCESS/FIRESTORE_DATAS/USERS\"\n",
    "# attend_file_names = [f for f in os.listdir(directory) if f.startswith('output-') and f.endswith('.json')]\n",
    "\n",
    "# Spark 세션 생성 및 전역 변수로 설정\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"JSON to DataFrame\") \\\n",
    "    .config(\"spark.driver.memory\", \"5g\") \\\n",
    "    .config(\"spark.executor.memory\", \"5g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"3g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86b64397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_to_df(directory):\n",
    "    \"\"\"해당 디렉토리로부터 'output-'형식의 JSON 파일들을 DataFrame으로 로드\"\"\"\n",
    "    file_names = [f for f in os.listdir(directory) if f.startswith('output-') and f.endswith('.json')]\n",
    "#     file_names = [f\"output-{i}.json\" for i in range(11)]\n",
    "    \n",
    "    dfs_chunk = []  # 이 리스트는 한 파일의 모든 청크 데이터프레임을 저장합니다.\n",
    "\n",
    "    for file_name in file_names:\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "            \n",
    "            users_data = data.get(\"Users\", {})\n",
    "            \n",
    "            for user_id, user_info in users_data.items():\n",
    "                attend_data = user_info.get(\"Attend\", {})\n",
    "                for attend_name, attend_info in attend_data.items():\n",
    "                    attend_df = pd.DataFrame([attend_info])  # Convert each attend_info into a dataframe\n",
    "                    attend_df['user_id'] = user_id  # Adding user ID\n",
    "                    dfs_chunk.append(attend_df)\n",
    "                    \n",
    "            # Log to check the processed files\n",
    "            print(f\"Processed file: {file_name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file: {file_name}. Reason: {str(e)}. Skipping this file.\")\n",
    "\n",
    "    # Concatenating all dataframes from all files\n",
    "    if dfs_chunk:\n",
    "        final_df = pd.concat(dfs_chunk, ignore_index=True)\n",
    "        return final_df\n",
    "    else:\n",
    "        print(\"No dataframes to concatenate.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8bc7272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def chunk_files(file_list, size):\n",
    "#     \"\"\"주어진 파일 리스트를 그룹화합니다.\"\"\"\n",
    "#     return [file_list[i:i + size] for i in range(0, len(file_list), size)]\n",
    "\n",
    "def save_progress(part_number):\n",
    "    \"\"\"작업의 진행 상황을 저장합니다.\"\"\"\n",
    "    directory = 'D:/DATA_PREPROCESS/FIRESTORE_DATAS/data_progress'\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    with open(os.path.join(directory, 'progress.txt'), 'w') as file:\n",
    "        file.write(str(part_number))\n",
    "\n",
    "def load_progress():\n",
    "    \"\"\"작업의 진행 상황을 불러옵니다.\"\"\"\n",
    "    if os.path.exists('D:/DATA_PREPROCESS/FIRESTORE_DATAS/data_progress/progress.txt'):\n",
    "        with open('D:/DATA_PREPROCESS/FIRESTORE_DATAS/data_progress/progress.txt', 'r') as file:\n",
    "            content = file.read().strip()\n",
    "            if content.isdigit():  # 파일의 내용이 숫자인지 확인\n",
    "                return int(content)\n",
    "    return 0  # 진행 상황 파일이 없으면 처음부터 시작\n",
    "\n",
    "# def process_json_file(file_path):\n",
    "#     # JSON 파일을 Spark DataFrame으로 읽기\n",
    "#     df = spark.read.option(\"multiLine\", \"true\").json(file_path)\n",
    "    \n",
    "#     # Users 딕셔너리를 배열로 변환하고 explode 함수 적용\n",
    "#     exploded_df = df.select(F.explode(F.col(\"Users\"))).toDF(\"user_id\", \"user_info\")\n",
    "    \n",
    "#     # Attend 정보를 추출합니다.\n",
    "#     attend_df = exploded_df.select(\n",
    "#         \"user_id\", \n",
    "#         F.explode(\"user_info.Attend\").alias(\"attend_name\", \"attend_info\"),\n",
    "#         F.col(\"attend_info.ArrayVoice\").alias(\"ArrayVoice\"),\n",
    "#         F.col(\"attend_info.attend\").alias(\"attend\"),\n",
    "#         F.col(\"attend_info.resultPass\").alias(\"resultPass\"),\n",
    "#         F.col(\"attend_info.imageUrl\").alias(\"imageUrl\"),\n",
    "#         F.col(\"attend_info.addText\").alias(\"addText\"),\n",
    "#         F.col(\"attend_info.name\").alias(\"name\"),\n",
    "#         F.col(\"attend_info.ArrayPercent\").alias(\"ArrayPercent\"),\n",
    "#         F.col(\"attend_info.ArrayDate\").alias(\"ArrayDate\"),\n",
    "#         F.col(\"attend_info.title\").alias(\"title\"),\n",
    "#         F.col(\"attend_info.type\").alias(\"type\"),\n",
    "#         F.col(\"attend_info.review_wish\").alias(\"review_wish\"),\n",
    "#         F.col(\"attend_info.ArrayAddResult\").alias(\"ArrayAddResult\")\n",
    "#     )\n",
    "    \n",
    "#     return attend_df\n",
    "def process_json_file_spark(file_path):\n",
    "    \"\"\"주어진 JSON 파일 경로에서 Users 및 Attend 정보를 처리하여 Spark DataFrame으로 반환합니다.\"\"\"\n",
    "    # JSON 파일을 Spark DataFrame으로 읽기\n",
    "    df = spark.read.option(\"multiLine\", \"true\").json(file_path)\n",
    "    \n",
    "    # Users 딕셔너리를 배열로 변환하고 explode 함수 적용\n",
    "    exploded_users_df = df.select(F.explode(F.col(\"Users\"))).toDF(\"user_id\", \"user_info\")\n",
    "    \n",
    "    # Attend 정보 추출\n",
    "    attend_df = exploded_users_df.select(\n",
    "        \"user_id\", \n",
    "        F.explode(\"user_info.Attend\").alias(\"attend_name\", \"attend_info\")\n",
    "    )\n",
    "    \n",
    "    # 필요한 컬럼 추출\n",
    "    final_df = attend_df.select(\n",
    "        \"user_id\",\n",
    "        \"attend_name\",\n",
    "        F.col(\"attend_info.ArrayVoice\").alias(\"ArrayVoice\"),\n",
    "        F.col(\"attend_info.attend\").alias(\"attend\"),\n",
    "        F.col(\"attend_info.resultPass\").alias(\"resultPass\"),\n",
    "        F.col(\"attend_info.imageUrl\").alias(\"imageUrl\"),\n",
    "        F.col(\"attend_info.addText\").alias(\"addText\"),\n",
    "        F.col(\"attend_info.name\").alias(\"name\"),\n",
    "        F.col(\"attend_info.ArrayPercent\").alias(\"ArrayPercent\"),\n",
    "        F.col(\"attend_info.ArrayDate\").alias(\"ArrayDate\"),\n",
    "        F.col(\"attend_info.title\").alias(\"title\"),\n",
    "        F.col(\"attend_info.type\").alias(\"type\"),\n",
    "        F.col(\"attend_info.review_wish\").alias(\"review_wish\"),\n",
    "        F.col(\"attend_info.ArrayAddResult\").alias(\"ArrayAddResult\")\n",
    "    )\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d06f6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_json_to_df_spark(directory: str) -> pd.DataFrame:\n",
    "#     file_names = [f for f in os.listdir(directory) if f.startswith('output-') and f.endswith('.json')]\n",
    "#     final_dfs = []\n",
    "    \n",
    "#     last_completed_part = load_progress()\n",
    "\n",
    "#     for idx, file_name in enumerate(file_names):\n",
    "#         file_path = os.path.join(directory, file_name)\n",
    "#         current_df = process_json_file(file_path)\n",
    "        \n",
    "#         temp_dfs = []\n",
    "#         user_ids = current_df.select(\"user_id\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "        \n",
    "#         for i in range(0, len(user_ids), 100):\n",
    "#             current_users = user_ids[i:i+100]\n",
    "#             chunk_df = current_df.filter(current_df.user_id.isin(current_users))\n",
    "#             temp_dfs.append(chunk_df)\n",
    "            \n",
    "#             # 중간 결과 저장\n",
    "#             if (idx * len(user_ids) + i) > last_completed_part:\n",
    "#                 final_dfs.append(chunk_df)\n",
    "#                 save_progress(idx * len(user_ids) + i)\n",
    "                \n",
    "#     final_df = reduce(DataFrame.unionByName, final_dfs)\n",
    "#     return final_df.toPandas()\n",
    "def load_json_to_df_spark(directory):\n",
    "    \"\"\"해당 디렉토리로부터 'output-'형식의 JSON 파일들을 DataFrame으로 로드\"\"\"\n",
    "    file_names = [f for f in os.listdir(directory) if f.startswith('output-') and f.endswith('.json')]\n",
    "    \n",
    "    # 진행 상황 로딩\n",
    "    last_completed_file_index = load_progress()\n",
    "    \n",
    "    # 모든 파일에 대해 처리 후 결과 DataFrame들을 병합\n",
    "    all_dfs = []\n",
    "    for idx, file_name in enumerate(file_names):\n",
    "        # 이전에 완료된 파일은 스킵\n",
    "        if idx < last_completed_file_index:\n",
    "            continue\n",
    "            \n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        current_df = process_json_file_spark(file_path)\n",
    "        all_dfs.append(current_df)\n",
    "        \n",
    "        # 현재 파일 인덱스를 진행 상황으로 저장\n",
    "        save_progress(idx + 1)\n",
    "\n",
    "    final_df = reduce(lambda x, y: x.union(y), all_dfs)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23e9d4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_to_df_with_dask(directory):\n",
    "    # DASK 작업이 분산 스케줄러에 의해 처리되게 함\n",
    "    client = Client(memory_limit='4GB')  # 시스템 메모리의 4GB를 사용하도록 설정\n",
    "    \n",
    "    file_names = [os.path.join(directory, f) for f in os.listdir(directory) if f.startswith('output-') and f.endswith('.json')]\n",
    "#     file_names = [os.path.join(directory, f) for f in os.listdir(directory) \n",
    "#               if f.startswith('output-') and f.endswith('.json') \n",
    "#               and 0 <= int(f.split('-')[1].split('.')[0]) <= 10]\n",
    "    \n",
    "    # JSON 파일 로드 및 전처리를 위한 함수\n",
    "    def process_file(file_path):\n",
    "        dfs_chunk = []\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "            users_data = data.get(\"Users\", {})\n",
    "\n",
    "            for user_id, user_info in users_data.items():\n",
    "                attend_data = user_info.get(\"Attend\", {})\n",
    "                for attend_name, attend_info in attend_data.items():\n",
    "                    attend_df = dd.from_pandas(pd.DataFrame([attend_info]), npartitions=1)  # Convert each attend_info into a dask dataframe\n",
    "                    attend_df['user_id'] = user_id\n",
    "                    dfs_chunk.append(attend_df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file: {file_name}. Reason: {str(e)}. Skipping this file.\")\n",
    "        return dfs_chunk\n",
    "\n",
    "    # 병렬 처리로 파일 읽기\n",
    "    bag = db.from_sequence(file_names).map(process_file).compute()\n",
    "    computed_bag = bag.compute()  # 이 부분에서 계산을 시작합니다.\n",
    "    progress(computed_bag)  # 진척상황을 표시합니다.\n",
    "\n",
    "    # 결과 데이터프레임 생성\n",
    "    if computed_bag:\n",
    "        final_df = dd.concat([chunk for sublist in computed_bag for chunk in sublist], axis=0)\n",
    "        return final_df\n",
    "    else:\n",
    "        print(\"No dataframes to concatenate.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "456e3457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_json_data_loading(directory, df):\n",
    "    \"\"\"데이터가 올바르게 DataFrame에 로드되었는지 테스트\"\"\"\n",
    "    file_names = [f for f in os.listdir(directory) if f.startswith('output-') and f.endswith('.json')]\n",
    "#     file_names = [f\"output-{i}.json\" for i in range(11)]\n",
    "    \n",
    "    total_attend_count = 0\n",
    "    \n",
    "    for file_name in file_names:\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "            users = data.get(\"Users\", {})\n",
    "            for user_data in users.values():\n",
    "                attend_data = user_data.get(\"Attend\", {})\n",
    "                total_attend_count += len(attend_data)\n",
    "#     # 단일 파일로 테스트 용\n",
    "#     with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#         data = json.load(file)\n",
    "#     users = data.get(\"Users\", {})\n",
    "#     for user_data in users.values():\n",
    "#         attend_data = user_data.get(\"Attend\", {})\n",
    "#         total_attend_count += len(attend_data)\n",
    "    \n",
    "    # Comparing the attend count in the files with the dataframe\n",
    "    df_attend_count = df.shape[0]\n",
    "    \n",
    "    assert total_attend_count == df_attend_count, f\"Attend count mismatch. Files: {total_attend_count}, DataFrame: {df_attend_count}\"\n",
    "    \n",
    "    return \"Test passed: Data loaded correctly into DataFrame.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d89d3694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_dataframe(df):\n",
    "    \"\"\"\n",
    "    주어진 Dataframe의 구조와 몇몇 데이터를 보여준다\n",
    "    \n",
    "    Args:\n",
    "    - df (DataFrame): 조사할 dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 첫 5행 확인\n",
    "    print(\"First 5 rows of the dataframe:\")\n",
    "    print(df.head())\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # 데이터프레임 정보 확인\n",
    "    print(\"\\nDataframe Info:\")\n",
    "    print(df.info())\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # 기술 통계 확인 (숫자 데이터에만 해당)\n",
    "    print(\"\\nDataframe Description:\")\n",
    "    print(df.describe())\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9992410f",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o32.json.\n: java.lang.OutOfMemoryError: Java heap space\r\n\tat scala.LowPriorityImplicits.wrapRefArray(Predef.scala:622)\r\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.children(TreeNode.scala:1246)\r\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.children$(TreeNode.scala:1246)\r\n\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.children$lzycompute(Expression.scala:532)\r\n\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.children(Expression.scala:532)\r\n\tat org.apache.spark.sql.catalyst.expressions.Expression._references$lzycompute(Expression.scala:125)\r\n\tat org.apache.spark.sql.catalyst.expressions.Expression._references(Expression.scala:124)\r\n\tat org.apache.spark.sql.catalyst.expressions.Expression.references(Expression.scala:127)\r\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$_references$1(Expression.scala:125)\r\n\tat org.apache.spark.sql.catalyst.expressions.Expression$$Lambda$2338/0x00000008018b77b8.apply(Unknown Source)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike$$Lambda$153/0x0000000800e04e78.apply(Unknown Source)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\r\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\r\n\tat org.apache.spark.sql.catalyst.expressions.Expression._references$lzycompute(Expression.scala:125)\r\n\tat org.apache.spark.sql.catalyst.expressions.Expression._references(Expression.scala:124)\r\n\tat org.apache.spark.sql.catalyst.expressions.Expression.references(Expression.scala:127)\r\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$_references$1(Expression.scala:125)\r\n\tat org.apache.spark.sql.catalyst.expressions.Expression$$Lambda$2338/0x00000008018b77b8.apply(Unknown Source)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike$$Lambda$153/0x0000000800e04e78.apply(Unknown Source)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# attend 정보가 들어간 json 파일들을 df_attend에 로드\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# df_attend = load_json_to_df(attend_directory)\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m df_attend \u001b[38;5;241m=\u001b[39m \u001b[43mload_json_to_df_spark\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattend_directory\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 41\u001b[0m, in \u001b[0;36mload_json_to_df_spark\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     40\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, file_name)\n\u001b[1;32m---> 41\u001b[0m current_df \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_json_file_spark\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m all_dfs\u001b[38;5;241m.\u001b[39mappend(current_df)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# 현재 파일 인덱스를 진행 상황으로 저장\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 52\u001b[0m, in \u001b[0;36mprocess_json_file_spark\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"주어진 JSON 파일 경로에서 Users 및 Attend 정보를 처리하여 Spark DataFrame으로 반환합니다.\"\"\"\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# JSON 파일을 Spark DataFrame으로 읽기\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmultiLine\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Users 딕셔너리를 배열로 변환하고 explode 함수 적용\u001b[39;00m\n\u001b[0;32m     55\u001b[0m exploded_users_df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mselect(F\u001b[38;5;241m.\u001b[39mexplode(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\u001b[38;5;241m.\u001b[39mtoDF(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_info\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\my_conda_01\\lib\\site-packages\\pyspark\\sql\\readwriter.py:418\u001b[0m, in \u001b[0;36mDataFrameReader.json\u001b[1;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, allowNonNumericNumbers)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[0;32m    421\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator: Iterable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterable:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\my_conda_01\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\my_conda_01\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\my_conda_01\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o32.json.\n: java.lang.OutOfMemoryError: Java heap space\r\n\tat scala.LowPriorityImplicits.wrapRefArray(Predef.scala:622)\r\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.children(TreeNode.scala:1246)\r\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.children$(TreeNode.scala:1246)\r\n\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.children$lzycompute(Expression.scala:532)\r\n\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.children(Expression.scala:532)\r\n\tat org.apache.spark.sql.catalyst.expressions.Expression._references$lzycompute(Expression.scala:125)\r\n\tat org.apache.spark.sql.catalyst.expressions.Expression._references(Expression.scala:124)\r\n\tat org.apache.spark.sql.catalyst.expressions.Expression.references(Expression.scala:127)\r\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$_references$1(Expression.scala:125)\r\n\tat org.apache.spark.sql.catalyst.expressions.Expression$$Lambda$2338/0x00000008018b77b8.apply(Unknown Source)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike$$Lambda$153/0x0000000800e04e78.apply(Unknown Source)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\r\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\r\n\tat org.apache.spark.sql.catalyst.expressions.Expression._references$lzycompute(Expression.scala:125)\r\n\tat org.apache.spark.sql.catalyst.expressions.Expression._references(Expression.scala:124)\r\n\tat org.apache.spark.sql.catalyst.expressions.Expression.references(Expression.scala:127)\r\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$_references$1(Expression.scala:125)\r\n\tat org.apache.spark.sql.catalyst.expressions.Expression$$Lambda$2338/0x00000008018b77b8.apply(Unknown Source)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike$$Lambda$153/0x0000000800e04e78.apply(Unknown Source)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\r\n"
     ]
    }
   ],
   "source": [
    "# attend 정보가 들어간 json 파일들을 df_attend에 로드\n",
    "# df_attend = load_json_to_df(attend_directory)\n",
    "df_attend = load_json_to_df_spark(attend_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39bd6fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1499"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6202651",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_attend' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# df_attend에 데이터가 올바르게 로드되었는지 test\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m test_result \u001b[38;5;241m=\u001b[39m test_json_data_loading(attend_directory, \u001b[43mdf_attend\u001b[49m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(test_result)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# inspect_dataframe(df_attend)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_attend' is not defined"
     ]
    }
   ],
   "source": [
    "# df_attend에 데이터가 올바르게 로드되었는지 test\n",
    "test_result = test_json_data_loading(attend_directory, df_attend)\n",
    "print(test_result)\n",
    "# inspect_dataframe(df_attend)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11528088",
   "metadata": {},
   "source": [
    "# 외부 디스크 데이터, df_external로 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26576bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_file_path = 'C:/Users/admin/Desktop/230517_d_95_extracted_ssd_d95_data_1.xlsx'  # 모든 외부 디스크\n",
    "external_file_path1 = 'C:/Users/admin/Desktop/230517_d_80_gcs_extracted_hdd_d80_data_1.xlsx' \n",
    "external_file_path2 = 'C:/Users/admin/Desktop/230517_d_80_gcs_extracted_hdd_d80_data_2.xlsx'\n",
    "\n",
    "df_ssd = pd.read_excel(file_path)   # 외부 디스크 통합\n",
    "df_hdd1 = pd.read_excel(file_path)  \n",
    "df_hdd2 = pd.read_excel(file_path1)\n",
    "df_external = pd.concat([df_ssd, df_hdd1, df_hdd2], ignore_index=True)\n",
    "\n",
    "num_rows = df.shape[0]\n",
    "num_columns = df.shape[1]\n",
    "print(f\"File has \\'{num_rows}\\' rows and \\'{num_columns}\\' columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d74a67",
   "metadata": {},
   "source": [
    "# 유저 정보 데이터, df_users로 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a182d669",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_file_path = 'D:/DATA_PREPROCESS/Firestore_datas/users_data.csv'\n",
    "\n",
    "df_users = pd.read_csv(users_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8758515e",
   "metadata": {},
   "source": [
    "# 포인트 광고 / 암기플러스 광고 데이터, df_ad_point / df_ad_memo로 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d8d939",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from google.oauth2.service_account import Credentials\n",
    "\n",
    "# Google Cloud Console에 대한 서비스 키\n",
    "creds_path = \"C:/Users/admin/Desktop/voice_data_queries/samboss-reward-394470968e63.json\"\n",
    "\n",
    "# Spreadsheet ID는 해당 url 주소의 d와 edit 사이의 값\n",
    "# spreadsheet_id = \"1XMjIScbWqyiyzGMfOk5jvbAQ_ys8Srg8e60R6BkqMJ4\"  # fs_point_20230801 시트\n",
    "memo_sheet_id = \"1fstoqfCCyIv38D4ZOxicoDdIaLhcfuXOf_9yn-ijn_U\"   # fs_memo_20230801 시트\n",
    "\n",
    "# Google Drive API와 Google Sheets API에 연결하고 인증\n",
    "creds = Credentials.from_service_account_file(creds_path)\n",
    "drive_service = build('drive', 'v3', credentials=creds)     # ( 사용하려는 API, API 버전, 인증 )\n",
    "sheets_service = build('sheets', 'v4', credentials=creds)\n",
    "\n",
    "# Spreadsheet에 대한 메타데이터\n",
    "memo_sheet_metadata = sheets_service.spreadsheets().get(spreadsheetId=memo_sheet_id).execute()\n",
    "\n",
    "# 시트의 이름을 지정 (예: 'Sheet1')\n",
    "sheet_name = '시트1'\n",
    "\n",
    "# # 범위를 지정 (예: 'Sheet1!A1:D10')\n",
    "# range_name = f'{sheet_name}!A1:C10'\n",
    "range_name = sheet_name  # 범위를 시트 이름만으로 지정 -> 전체 시트\n",
    "\n",
    "# 시트의 내용 가져오기\n",
    "memo_sheet = sheets_service.spreadsheets().values().get(spreadsheetId=memo_sheet_id, range=range_name).execute()\n",
    "\n",
    "# 결과를 Pandas DataFrame으로 변환\n",
    "df_ad_memo = pd.DataFrame(memo_sheet.get('values', []))\n",
    "\n",
    "# 첫 번째 행을 열 이름으로 사용\n",
    "df.columns = df.iloc[0]\n",
    "df = df.iloc[1:]\n",
    "\n",
    "# 결과 확인\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
