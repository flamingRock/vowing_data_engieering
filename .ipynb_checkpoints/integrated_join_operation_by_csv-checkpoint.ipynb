{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cebf26b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, isnull, count, when, substring, coalesce\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32522b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark 세션 생성\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CSV to DataFrame\") \\\n",
    "    .config(\"spark.driver.memory\", \"5g\") \\\n",
    "    .config(\"spark.executor.memory\", \"5g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"3g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "086aa12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_to_df_spark(directory, file_names):\n",
    "    \"\"\"해당 디렉토리로부터 지정된 형식의 CSV 파일들을 DataFrame으로 로드\"\"\"\n",
    "    \n",
    "    all_dfs = []\n",
    "    for idx, file_name in enumerate(file_names):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        current_df = spark.read.option(\"multiline\", \"true\").csv(file_path, header=True, inferSchema=True)\n",
    "        all_dfs.append(current_df)\n",
    "\n",
    "    # reduce는 순차적인 데이터에 누적 연산, union은 병합 함수, 여기선 모든 df에 대해 순차적으로 병합하는 연산\n",
    "    final_df = reduce(lambda x, y: x.union(y), all_dfs)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ec09dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "attend_directory = \"D:/DATA_PREPROCESS/FIRESTORE_DATAS/USERS\"\n",
    "attend_file_names = [f for f in os.listdir(attend_directory) if f.startswith('output-') and f.endswith('.csv')]\n",
    "df_attend = load_csv_to_df_spark(attend_directory, attend_file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d132bfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        user_id        ArrayVoice attend resultPass  \\\n",
      "0  2A0dHXWyfYfyEbaHVsOiYA8Hlnr2        ['화이트 모던']      1       pass   \n",
      "1  2A0dHXWyfYfyEbaHVsOiYA8Hlnr2  ['멋스러운 컬러 인테리어']      1       pass   \n",
      "2  2A0dHXWyfYfyEbaHVsOiYA8Hlnr2       ['깔끔한 디자인']      1       pass   \n",
      "3  2A0dHXWyfYfyEbaHVsOiYA8Hlnr2       ['모던한 디자인']      1       pass   \n",
      "4  2A0dHXWyfYfyEbaHVsOiYA8Hlnr2      ['모던심플 스타일']      1       pass   \n",
      "\n",
      "                                            imageUrl    addText  \\\n",
      "0  https://firebasestorage.googleapis.com/v0/b/sa...      화이트모던   \n",
      "1  https://firebasestorage.googleapis.com/v0/b/sa...  멋스런컬러인테리어   \n",
      "2  https://firebasestorage.googleapis.com/v0/b/sa...     깔끔한디자인   \n",
      "3  https://firebasestorage.googleapis.com/v0/b/sa...     모던한디자인   \n",
      "4  https://firebasestorage.googleapis.com/v0/b/sa...    모던심플스타일   \n",
      "\n",
      "                                                name ArrayPercent  \\\n",
      "0  미자크 데일리 1900 주방 수납장 - 냉장고 키큰 부엌 틈새 팬트리 빌트인 장 자...      ['100']   \n",
      "1  미자크 리버스 틈새 주방 수납장 냉장고자리 부엌 팬트리 속깊은 빌트인 키큰 장 비스...       ['80']   \n",
      "2  미자크 리버스 틈새 주방 수납장 냉장고자리 부엌 팬트리 속깊은 빌트인 키큰 장 비스...      ['100']   \n",
      "3  미자크 리버스 틈새 주방 수납장 냉장고자리 부엌 팬트리 속깊은 빌트인 키큰 장 비스...      ['100']   \n",
      "4  미자크 리버스 틈새 주방 수납장 냉장고자리 부엌 팬트리 속깊은 빌트인 키큰 장 비스...      ['100']   \n",
      "\n",
      "                 ArrayDate  title  type review_wish ArrayAddResult  \n",
      "0  ['23.07.24 오전 12시 34분']  암기플러스  True           0      ['화이트모던']  \n",
      "1  ['23.06.20 오후 02시 23분']  암기플러스  True           0  ['멋스런컬러인테리어']  \n",
      "2  ['23.06.21 오후 01시 13분']  암기플러스  True           0     ['깔끔한디자인']  \n",
      "3  ['23.06.22 오전 07시 53분']  암기플러스  True           0     ['모던한디자인']  \n",
      "4  ['23.06.23 오전 07시 32분']  암기플러스  True           0    ['모던심플스타일']  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['user_id',\n",
       " 'ArrayVoice',\n",
       " 'attend',\n",
       " 'resultPass',\n",
       " 'imageUrl',\n",
       " 'addText',\n",
       " 'name',\n",
       " 'ArrayPercent',\n",
       " 'ArrayDate',\n",
       " 'title',\n",
       " 'type',\n",
       " 'review_wish',\n",
       " 'ArrayAddResult']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)  # 모든 열을 표시\n",
    "pd.set_option('display.width', None)        # 화면 너비에 맞게 출력\n",
    "print(df_attend.limit(5).toPandas())\n",
    "# df_attend.count()\n",
    "df_attend.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e4c4016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_memo 행 개수: 5299 df_point 행 개수: 1180\n",
      "df_ad의 행 개수: 6479 열 개수: 30\n"
     ]
    }
   ],
   "source": [
    "firestore_directory = \"D:/DATA_PREPROCESS/FIRESTORE_DATAS\"\n",
    "df_memo = load_csv_to_df_spark(firestore_directory, ['fs_memo_20230803.csv'])\n",
    "df_point = load_csv_to_df_spark(firestore_directory, ['fs_point_20230801.csv'])\n",
    "\n",
    "print(f\"df_memo 행 개수: {df_memo.count()} df_point 행 개수: {df_point.count()}\")\n",
    "\n",
    "\n",
    "# # 합치기 전에 타입 확인하기 -> point와 memo의 동일 column의 타입들이 달랐다\n",
    "# print(\"df_memo columns:\")\n",
    "# for col, dtype in df_memo.dtypes:\n",
    "#     print(f\"{col}: {dtype}\")\n",
    "# print(\"\\ndf_point_dropped columns:\")\n",
    "# for col, dtype in df_point.dtypes:\n",
    "#     print(f\"{col}: {dtype}\")\n",
    "\n",
    "# df_point의 데이터 타입 변경\n",
    "df_point = df_point.withColumn(\"linkClick\", df_point[\"linkClick\"].cast(\"int\")) \\\n",
    "                   .withColumn(\"linkClickDesc\", df_point[\"linkClickDesc\"].cast(\"int\")) \\\n",
    "                   .withColumn(\"linkWing\", df_point[\"linkWing\"].cast(\"boolean\")) \\\n",
    "                   .withColumn(\"pay\", df_point[\"pay\"].cast(\"int\")) \\\n",
    "                   .withColumn(\"percent\", df_point[\"percent\"].cast(\"int\")) \\\n",
    "                   .withColumn(\"player\", df_point[\"player\"].cast(\"int\")) \\\n",
    "                   .withColumn(\"point\", df_point[\"point\"].cast(\"int\")) \\\n",
    "                   .withColumn(\"type\", df_point[\"type\"].cast(\"boolean\"))\n",
    "\n",
    "# 'quiz' 컬럼 제거\n",
    "df_point_dropped = df_point.drop('quiz')\n",
    "\n",
    "# 데이터프레임 합치기\n",
    "df_ad = df_memo.union(df_point_dropped)\n",
    "\n",
    "# 결과 확인\n",
    "print(f\"df_ad의 행 개수: {df_ad.count()} 열 개수: {len(df_ad.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68a038da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------------+--------------+--------------------+------+----------+----------+-------+\n",
      "|accuracy|                       ad_name|   record_time|             user_id|gender|birth_year|local_code|is_test|\n",
      "+--------+------------------------------+--------------+--------------------+------+----------+----------+-------+\n",
      "|      80|'섬·바다 여권’ 들고 여행을 ...|20221101162726|icVEsPDs4ia0m9Jjx...|     F|        89|        Gy|      0|\n",
      "|      80|'섬·바다 여권’ 들고 여행을 ...|20221101162949|G5ultYhAslNplMxW0...|     M|         0|        EE|      1|\n",
      "|      80|'섬·바다 여권’ 들고 여행을 ...|20221101180026|4uNFklxemxO3dnKL5...|     F|        91|        Ch|      0|\n",
      "|      80|'섬·바다 여권’ 들고 여행을 ...|20221101180216|puDQ5R5fRba59WJFd...|     F|        91|        Se|      0|\n",
      "|      80|'섬·바다 여권’ 들고 여행을 ...|20221101180549|BG8E6coISdOvISp84...|     F|        63|        Gy|      0|\n",
      "+--------+------------------------------+--------------+--------------------+------+----------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1128340"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_ssd = load_csv_to_df_spark(firestore_directory, ['230517_d_95_extracted_ssd_d95_data_1.csv'])\n",
    "df_hdd = load_csv_to_df_spark(firestore_directory, ['230517_d_80_gcs_extracted_hdd_d80_data_1.csv', '230517_d_80_gcs_extracted_hdd_d80_data_2.csv'])\n",
    "# df_externals = df_ssd.union(df_hdd)\n",
    "df_externals = df_hdd\n",
    "df_externals.show(5)\n",
    "df_externals.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5a058e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+--------+--------------+----+--------+------------+--------+----------+-------------------------+------------+--------------+------+-------+--------------+------------+-----+---------------+---------+---------------+-------------+--------+----+----------+--------------------+---+--------------------+----------------+------+-------------------------------+-------------+----------+--------+--------------------+------------------+--------------------+-----------------+--------+--------------+----------------+----------+----+----+-----------+-----------------+-----------+----------+----------+-----------+-----+----+\n",
      "|REVIEW_TOPIC|auth|birthday|      wingTime|wing|realName|PRIZES_TOPIC|    name|     phone|isChallengeRequestAllowed|monthlyPoint| detailAddress|engine|perfect|isSoundAllowed|NOTICE_TOPIC|point|monthlyPerfects|ANY_TOPIC|MARKETING_TOPIC|searchKeyWord|language| job|      date|               image|win|                mail|         phoneId|gender|                        address|recommendCode|isProvider|isOnline|             user_id|previousMonthPoint|              userID|lastParticipation|fcmToken|monthlyPerfect|challengeRequest|gamePlayed|city|lost|dailyPoint2|previousWeekPoint|weeklyPoint|spentHeart|dailyPoint|spentHeart2|right|best|\n",
      "+------------+----+--------+--------------+----+--------+------------+--------+----------+-------------------------+------------+--------------+------+-------+--------------+------------+-----+---------------+---------+---------------+-------------+--------+----+----------+--------------------+---+--------------------+----------------+------+-------------------------------+-------------+----------+--------+--------------------+------------------+--------------------+-----------------+--------+--------------+----------------+----------+----+----+-----------+-----------------+-----------+----------+----------+-----------+-----+----+\n",
      "|       false|   0|19980902|20230724173311|   0|  이로아|       false|  이로아|1032025014|                     true|         120|분당서울대병원|수도권|    0.0|          true|       false|  120|            0.0|    false|          false|           []|대한민국|기타|2023-07-24|https://firebases...|  0| floah0902@naver.com|69dd2c16c78f5a01|  여자|경기 성남시 분당구 구미로173...|             |     email|   false|003MZepVQAdiNf4F8...|              null|                null|             null|    null|          null|            null|      null|null|null|       null|             null|       null|      null|      null|       null| null|null|\n",
      "|        true|   0|19790502|20230825015956|  10|  류미혜|        true|  자스민|1097227767|                     true|           0|          null|수도권|    0.0|          true|        true|    0|            0.0|     true|           true|           []|대한민국|null|2023-08-25|https://firebases...|  0| jasmin502@naver.com|9b50662ac86138d5|  여자|                           null|             |     email|   false|003prDgR8vXLxEQ2u...|              null|                null|             null|    null|          null|            null|      null|null|null|       null|             null|       null|      null|      null|       null| null|null|\n",
      "|       false|   0|20031107|20230801231932|   7| 권규민 |       false|    넌거|1024560974|                     true|          55|  102동 2313호|경상권|    0.0|          true|       false|   55|            0.0|    false|          false|           []|대한민국|학생|2023-08-01|https://firebases...|  0|1104flower@naver.com|08c26a290dd1542f|  남자|  부산 남구 용호로42번길 14 ...|             |     email|   false|00AqDjYcRpeNqI8nb...|              null|                null|             null|    null|          null|            null|      null|null|null|       null|             null|       null|      null|      null|       null| null|null|\n",
      "|        null|   0|19711120|20220902164507|  15|  vowing|        null|vowing12|1000000000|                     true|        1290|             1|수도권|   null|          true|        null| 1290|           null|     null|           null|           []|대한민국|기타|2022-07-14|  images/profile.jpg|  0|   vowing@vowing.com|1a4d488227d3f054|  남자|                              1|             |     email|   false|00FuHOZSAXcqRrnkh...|               0.0|00FuHOZSAXcqRrnkh...|             null|    null|          null|            null|      null|null|null|       null|             null|       null|      null|      null|       null| null|null|\n",
      "|        true|   0|20051126|20230627141453|  10|  장혜정|        true| ihyfoll|1044218323|                     true|           0|   101동 306호|경상권|    0.0|          true|        true|    0|            0.0|     true|           true|           []|대한민국|학생|2023-06-27|https://lh3.googl...|  0| jhj832300@gmail.com|9a1ff124b4630e61|  여자|경남 합천군 합천읍 핫들1로 1...|             |      null|   false|00K1mCptgPPsyRgRM...|              null|                null|             null|    null|          null|            null|      null|null|null|       null|             null|       null|      null|      null|       null| null|null|\n",
      "+------------+----+--------+--------------+----+--------+------------+--------+----------+-------------------------+------------+--------------+------+-------+--------------+------------+-----+---------------+---------+---------------+-------------+--------+----+----------+--------------------+---+--------------------+----------------+------+-------------------------------+-------------+----------+--------+--------------------+------------------+--------------------+-----------------+--------+--------------+----------------+----------+----+----+-----------+-----------------+-----------+----------+----------+-----------+-----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "34539"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_users = load_csv_to_df_spark(firestore_directory, ['users_data.csv'])\n",
    "df_users.show(5)\n",
    "df_users.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01f26c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows where all columns (except user_id) are missing: 0\n"
     ]
    }
   ],
   "source": [
    "# df_users 불완전 데이터 체크\n",
    "\n",
    "# user_id를 제외한 모든 컬럼 리스트 생성\n",
    "columns_except_user_id = [c for c in df_users.columns if c != \"user_id\"]\n",
    "\n",
    "# 모든 해당 컬럼들이 null인 조건 생성\n",
    "condition = [isnull(column_name) for column_name in columns_except_user_id]\n",
    "combined_condition = condition[0]\n",
    "for c in condition[1:]:\n",
    "    combined_condition &= c\n",
    "\n",
    "# 조건에 맞는 행을 필터링하고 개수 확인\n",
    "missing_rows_count = df_users.filter(combined_condition).count()\n",
    "print(f\"Number of rows where all columns (except user_id) are missing: {missing_rows_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596b09eb",
   "metadata": {},
   "source": [
    "# df_attend & df_externals 조인 쿼리 수행\n",
    "### df_attend가 기준 스키마"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0598b6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 16dmLeRXkTTRLyN2mrffyYxEs5o2\n",
      "ArrayVoice: ['수영만 요트투어']\n",
      "attend: 1\n",
      "resultPass: pass\n",
      "imageUrl: https://firebasestorage.googleapis.com/v0/b/samboss-reward.appspot.com/o/ads_script%2F1%2Ft8.png?alt=media&token=b3bd5003-f4af-43e8-b845-6521af9279f2\n",
      "addText: 수영만요트투어\n",
      "name: [부산요트투어] 오직 우리만 타는 럭셔리 프라이빗 해운대 광안리 더베이101 파워요트 10967\n",
      "ArrayPercent: ['100']\n",
      "ArrayDate: ['23.07.10 오전 07시 52분']\n",
      "title: 암기플러스\n",
      "type: True\n",
      "review_wish: 0\n",
      "ArrayAddResult: ['수영만요트투어']\n",
      "record_time: None\n",
      "gender: None\n",
      "birth_year: None\n",
      "local_code: None\n",
      "is_test: None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['user_id',\n",
       " 'ArrayVoice',\n",
       " 'attend',\n",
       " 'resultPass',\n",
       " 'imageUrl',\n",
       " 'addText',\n",
       " 'name',\n",
       " 'ArrayPercent',\n",
       " 'ArrayDate',\n",
       " 'title',\n",
       " 'type',\n",
       " 'review_wish',\n",
       " 'ArrayAddResult',\n",
       " 'record_time',\n",
       " 'gender',\n",
       " 'birth_year',\n",
       " 'local_code',\n",
       " 'is_test']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_attend와 df_externals의 조인\n",
    "\n",
    "# df_attend에서 가져올 컬럼 선택\n",
    "selected_columns = [df_attend[\"user_id\"], df_attend[\"name\"]]\n",
    "for column in df_attend.columns:\n",
    "    if column not in [\"ArrayAddResult\", \"type\", \"review_wish\"]:\n",
    "        selected_columns.append(df_attend[column])\n",
    "        \n",
    "# df_externals에서 가져올 컬럼 추가\n",
    "for column in [\"birth_year\", \"local_code\", \"is_test\"]:\n",
    "    selected_columns.append(df_externals[column])\n",
    "\n",
    "# 조인 조건 설정\n",
    "join_condition_attend_externals = (df_attend[\"user_id\"] == df_externals[\"user_id\"]) & (df_attend[\"name\"] == df_externals[\"ad_name\"])\n",
    "\n",
    "# 조인 수행\n",
    "result_df_attend_externals = df_attend.join(df_externals, join_condition_attend_externals, \"left_outer\")\n",
    "\n",
    "# 중복된 컬럼 제거\n",
    "result_df_attend_externals = result_df_attend_externals.drop(df_externals[\"user_id\"]).drop(\"accuracy\").drop(df_externals[\"ad_name\"])\n",
    "\n",
    "\n",
    "first_row = result_df_attend_externals.head().asDict()\n",
    "for key, value in first_row.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "result_df_attend_externals.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c405e7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_duplicate_columns(df):\n",
    "#     columns = df.columns\n",
    "#     duplicates = [col for col in columns if columns.count(col) > 1]\n",
    "#     return list(set(duplicates))\n",
    "\n",
    "# duplicate_columns = find_duplicate_columns(result_df_attend_externals)\n",
    "# print(duplicate_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1d224f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender에 값이 있는 행의 개수: 1010475\n",
      "gender에 값이 없는 행의 개수: 5703556\n"
     ]
    }
   ],
   "source": [
    "# gender 값이 있는 행의 개수\n",
    "count_with_values = result_df_attend_externals.filter(result_df_attend_externals['gender'].isNotNull()).count()\n",
    "\n",
    "# gender 값이 없는 행의 개수\n",
    "count_without_values = result_df_attend_externals.filter(result_df_attend_externals['gender'].isNull()).count()\n",
    "\n",
    "print(f\"gender에 값이 있는 행의 개수: {count_with_values}\")\n",
    "print(f\"gender에 값이 없는 행의 개수: {count_without_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c93eab19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_externals에서 'birth_year', 'local_code', 'is_test' 중 하나라도 null이 아닌 행의 수: 1128340\n",
      "result_df_attend_externals에서 'birth_year', 'local_code', 'is_test' 중 하나라도 null이 아닌 행의 수: 1010475\n"
     ]
    }
   ],
   "source": [
    "non_null_externals_rows = df_externals.filter(\n",
    "    (df_externals[\"birth_year\"].isNotNull()) |\n",
    "    (df_externals[\"local_code\"].isNotNull()) |\n",
    "    (df_externals[\"is_test\"].isNotNull())\n",
    ").count()\n",
    "print(f\"df_externals에서 'birth_year', 'local_code', 'is_test' 중 하나라도 null이 아닌 행의 수: {non_null_externals_rows}\")\n",
    "\n",
    "# 2단계: result_df_attend_externals에서 'birth_year', 'local_code', 'is_test' 중 하나라도 null인 행의 수 확인\n",
    "non_null_result_rows = result_df_attend_externals.filter(\n",
    "    (result_df_attend_externals[\"birth_year\"].isNotNull()) |\n",
    "    (result_df_attend_externals[\"local_code\"].isNotNull()) |\n",
    "    (result_df_attend_externals[\"is_test\"].isNotNull())\n",
    ").count()\n",
    "\n",
    "print(f\"result_df_attend_externals에서 'birth_year', 'local_code', 'is_test' 중 하나라도 null이 아닌 행의 수: {non_null_result_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9e54fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching keys between df_attend and df_externals: 1010475\n"
     ]
    }
   ],
   "source": [
    "# df_attend와 df_externals에서 조인 조건에 맞는 키 값이 얼마나 많이 일치하는지 확인\n",
    "matching_keys_count = df_attend.join(df_externals, join_condition_attend_externals, \"inner\").count()\n",
    "print(f\"Matching keys between df_attend and df_externals: {matching_keys_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a52a783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_externals 내에서 user_id와 ad_name의 조합이 중복으로 나타나는 수: 5189\n",
      "df_externals 내에서 user_id와 ad_name, accuracy의 조합이 중복으로 나타나는 수: 5189\n",
      "df_externals 내에서 user_id, ad_name, 그리고 record_time의 조합이 중복으로 나타나는 수: 0\n",
      "df_externals 내에서 user_id, ad_name, 그리고 gender의 조합이 중복으로 나타나는 수: 5186\n",
      "df_externals 내에서 user_id, ad_name, 그리고 birth_year의 조합이 중복으로 나타나는 수: 5181\n",
      "df_externals 내에서 record_time을 제외한 파일명이 중복으로 나타나는 경우의 수: 5180\n"
     ]
    }
   ],
   "source": [
    "# df_externals 내에서 user_id와 ad_name의 조합이 중복으로 나타나는 경우의 수\n",
    "duplicate_keys_count = df_externals.groupBy(\"user_id\", \"ad_name\").agg(count(\"*\").alias(\"num_rows\")).filter(\"num_rows > 1\").count()\n",
    "print(f\"df_externals 내에서 user_id와 ad_name의 조합이 중복으로 나타나는 수: {duplicate_keys_count}\")\n",
    "\n",
    "# df_externals 내에서 user_id와 ad_name, accuracy의 조합이 중복으로 나타나는 경우의 수\n",
    "duplicate_keys_count = df_externals.groupBy(\"user_id\", \"ad_name\", \"accuracy\").agg(count(\"*\").alias(\"num_rows\")).filter(\"num_rows > 1\").count()\n",
    "print(f\"df_externals 내에서 user_id와 ad_name, accuracy의 조합이 중복으로 나타나는 수: {duplicate_keys_count}\")\n",
    "\n",
    "# df_externals 내에서 user_id와 ad_name, record_time의 조합이 중복으로 나타나는 경우의 수\n",
    "duplicate_keys_count = df_externals.groupBy(\"user_id\", \"ad_name\", \"record_time\").agg(count(\"*\").alias(\"num_rows\")).filter(\"num_rows > 1\").count()\n",
    "print(f\"df_externals 내에서 user_id, ad_name, 그리고 record_time의 조합이 중복으로 나타나는 수: {duplicate_keys_count}\")\n",
    "\n",
    "# df_externals 내에서 user_id와 ad_name, gender의 조합이 중복으로 나타나는 경우의 수\n",
    "duplicate_keys_count = df_externals.groupBy(\"user_id\", \"ad_name\", \"gender\").agg(count(\"*\").alias(\"num_rows\")).filter(\"num_rows > 1\").count()\n",
    "print(f\"df_externals 내에서 user_id, ad_name, 그리고 gender의 조합이 중복으로 나타나는 수: {duplicate_keys_count}\")\n",
    "\n",
    "# df_externals 내에서 user_id와 ad_name, birth_year의 조합이 중복으로 나타나는 경우의 수\n",
    "duplicate_keys_count = df_externals.groupBy(\"user_id\", \"ad_name\", \"birth_year\").agg(count(\"*\").alias(\"num_rows\")).filter(\"num_rows > 1\").count()\n",
    "print(f\"df_externals 내에서 user_id, ad_name, 그리고 birth_year의 조합이 중복으로 나타나는 수: {duplicate_keys_count}\")\n",
    "\n",
    "# df_externals 내에서 record_time을 제외한 파일명이 중복으로 나타나는 경우의 수\n",
    "duplicate_keys_count = df_externals.groupBy(\"user_id\", \"ad_name\", \"gender\", \"birth_year\", \"local_code\").agg(count(\"*\").alias(\"num_rows\")).filter(\"num_rows > 1\").count()\n",
    "print(f\"df_externals 내에서 record_time을 제외한 파일명이 중복으로 나타나는 경우의 수: {duplicate_keys_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4666f49e",
   "metadata": {},
   "source": [
    "## 일단 외부 저장소(SSD, HDD)에 데이터가 중복 저장되었다고 간주, 다음 작업으로 넘어간다\n",
    "\n",
    "df_externals 내에서 user_id와 ad_name의 조합이 중복으로 나타나는 수: 373451\n",
    "df_externals 내에서 user_id, ad_name, 그리고 gender의 조합이 중복으로 나타나는 수: 373450\n",
    "df_externals 내에서 user_id, ad_name, 그리고 birth_year의 조합이 중복으로 나타나는 수: 373446\n",
    "\n",
    "이 결과를 통해 user_id, ad_name, gender, birth_year 즉 SSD에 저장된 record_time이 없는 거의 대부분의 데이터가 HDD 데이터와 중복이 발생한다는 걸 확인할 수 있다.\n",
    "추가 작업을 통해 이 중복이 확실한 것인지 체크하고 관련해서 활동들이 필요하다\n",
    "\n",
    "user 수 := ssd u95 파일 수 := df_externals의 중복수 \n",
    "이 세가지의 수가 서로 근사치임을 참고\n",
    "\n",
    "! u80은 80퍼 이상을 모두 포함하니 u95는 당연히 모두 포함되잖아?! -> 그럼 df_externals에 ssd의 u95는 포함안시키는 게 맞네"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54259ad9",
   "metadata": {},
   "source": [
    "# result_df_attend_externals & df_users 조인 쿼리 수행\n",
    "### result_df_attend_externals가 기준 스키마"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f86d5fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+----------+--------------------+--------------+---------------------------------+------------+-----------------------+----------+----+-----------+------------------+-----------+------+----------+----------+-------+------+--------+-------+\n",
      "|             user_id|         ArrayVoice|attend|resultPass|            imageUrl|       addText|                             name|ArrayPercent|              ArrayDate|     title|type|review_wish|    ArrayAddResult|record_time|gender|birth_year|local_code|is_test|   job|language|perfect|\n",
      "+--------------------+-------------------+------+----------+--------------------+--------------+---------------------------------+------------+-----------------------+----------+----+-----------+------------------+-----------+------+----------+----------+-------+------+--------+-------+\n",
      "|16dmLeRXkTTRLyN2m...|['수영만 요트투어']|     1|      pass|https://firebases...|수영만요트투어| [부산요트투어] 오직 우리만 타...|     ['100']|['23.07.10 오전 07시...|암기플러스|True|          0|['수영만요트투어']|       null|     W|        73|        Gy|   null|  기타|대한민국| 3112.0|\n",
      "|2HhpEVZfVMexTLczi...|       ['임신준비']|     1|      pass|https://firebases...|      임신준비|     [평점 4.9] 콜린 이노시톨 ...|     ['100']|['23.07.29 오후 02시...|암기플러스|True|          0|      ['임신준비']|       null|     W|        82|        Se|   null|사무직|대한민국| 3766.0|\n",
      "|4QwLd6MnPpgNyT8xR...|         ['활꽃게']|     1|      pass|https://firebases...|        활꽃게|      서해안 제철 꽃게 1kg 2Kg...|     ['100']|['23.06.10 오후 12시...|암기플러스|True|          0|        ['활꽃게']|       null|     W|        68|        Se|   null|  기타|대한민국| 5837.0|\n",
      "|5V748lEzY9RjdQVof...|      ['호환 필터']|     1|      pass|https://firebases...|      호환필터|     [호환] LG 엘지 퓨리케어 3...|     ['100']|['23.06.07 오후 12시...|암기플러스|True|          0|      ['호환필터']|       null|     W|        70|        Se|   null|  기타|대한민국| 5574.0|\n",
      "|8ubJdQyQjfZmdJhlr...|      ['조립 선반']|     1|      pass|https://firebases...|      조립선반|암기플러스로 치매 예방하세요♡ ...|     ['100']|['23.08.09 오후 02시...|암기플러스|True|          0|      ['조립선반']|       null|     W|        78|        Se|   null|사무직|대한민국| 3323.0|\n",
      "+--------------------+-------------------+------+----------+--------------------+--------------+---------------------------------+------------+-----------------------+----------+----+-----------+------------------+-----------+------+----------+----------+-------+------+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df_users 변환\n",
    "df_users_transformed = df_users.withColumn(\"gender\", when(col(\"gender\") == \"남자\", \"M\").otherwise(\"W\")) \\\n",
    "                               .withColumn(\"birthday\", substring(col(\"birthday\"), 3, 2)) \\\n",
    "                               .withColumnRenamed(\"user_id\", \"user_id_in_users\") \\\n",
    "                               .withColumn(\"engine\", when(col(\"engine\") == \"수도권\", \"Se\")\n",
    "                                           .when(col(\"engine\") == \"경상권\", \"Gy\")\n",
    "                                           .when(col(\"engine\") == \"강원권\", \"Ga\")\n",
    "                                           .when(col(\"engine\") == \"충청권\", \"Ch\")\n",
    "                                           .when(col(\"engine\") == \"전라권\", \"Jd\")\n",
    "                                           .when(col(\"engine\") == \"제주권\", \"Je\")\n",
    "                                           .otherwise(\"EE\"))  # Jd가 전라도인지 제주도인지 불확실\n",
    "\n",
    "# df_users_transformed에서 필요한 컬럼만 선택\n",
    "df_users_transformed = df_users_transformed.select(\"user_id_in_users\", \"gender\", \"birthday\", \"engine\", \"job\", \"language\", \"perfect\")\n",
    "\n",
    "# 조인 조건 설정\n",
    "join_condition = (result_df_attend_externals[\"user_id\"] == df_users_transformed[\"user_id_in_users\"])\n",
    "\n",
    "# 조인 수행\n",
    "result_df_users = result_df_attend_externals.join(df_users_transformed, join_condition, \"left_outer\")\n",
    "\n",
    "# 필요한 컬럼만 선택하여 중복을 제거\n",
    "result_df_users = result_df_users.select(result_df_attend_externals[\"*\"],\n",
    "                                         df_users_transformed[\"gender\"].alias(\"new_gender\"),\n",
    "                                         df_users_transformed[\"birthday\"].alias(\"new_birthday\"),\n",
    "                                         df_users_transformed[\"engine\"].alias(\"new_engine\"),\n",
    "                                         df_users_transformed[\"job\"],\n",
    "                                         df_users_transformed[\"language\"],\n",
    "                                         df_users_transformed[\"perfect\"])\n",
    "\n",
    "# coalesce 함수를 사용하여 컬럼 값을 갱신\n",
    "result_df_users = result_df_users.withColumn(\"gender\", coalesce(col(\"new_gender\"), col(\"gender\"))) \\\n",
    "                                 .withColumn(\"birth_year\", coalesce(col(\"new_birthday\"), col(\"birth_year\"))) \\\n",
    "                                 .withColumn(\"local_code\", coalesce(col(\"new_engine\"), col(\"local_code\")))\n",
    "\n",
    "# 중복된 컬럼 및 임시 컬럼 제거\n",
    "result_df_users = result_df_users.drop(\"new_gender\", \"new_birthday\", \"new_engine\", \"user_id_in_users\")\n",
    "\n",
    "\n",
    "# 중복된 컬럼 제거\n",
    "result_df_users = result_df_users.drop(df_users_transformed[\"user_id_in_users\"])\n",
    "\n",
    "result_df_users.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb789b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|gender|  count|\n",
      "+------+-------+\n",
      "|     F|  23445|\n",
      "|  null|  83857|\n",
      "|     M|1405604|\n",
      "|     W|5201125|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 각 값에 따른 개수 출력\n",
    "gender_counts = result_df_users.groupBy(\"gender\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46e46005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArrayDate에 값이 있는 행의 개수: 6714030\n",
      "ArrayDate에 값이 없는 행의 개수: 1\n"
     ]
    }
   ],
   "source": [
    "# name 값이 있는 행의 개수\n",
    "name_present_count = result_df_users.filter(result_df_users.name.isNotNull()).count()\n",
    "\n",
    "# name 값이 없는 행의 개수\n",
    "name_absent_count = result_df_users.filter(result_df_users.name.isNull()).count()\n",
    "\n",
    "# # ArrayDate 값이 없으면서 record_time에 값이 있는 행의 개수\n",
    "# array_date_absent_record_time_present_count = result_df_users.filter(\n",
    "#     (result_df_users.ArrayDate.isNull()) & \n",
    "#     (result_df_users.record_time.isNotNull())\n",
    "# ).count()\n",
    "\n",
    "print(f\"ArrayDate에 값이 있는 행의 개수: {name_present_count}\")\n",
    "print(f\"ArrayDate에 값이 없는 행의 개수: {name_absent_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38e3f302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 행의 개수: 6714031\n"
     ]
    }
   ],
   "source": [
    "print(f\"전체 행의 개수: {result_df_users.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e893c4",
   "metadata": {},
   "source": [
    "# result_df_users & df_ad 조인 쿼리 수행\n",
    "\n",
    "### result_df_attend_externals가 기준 스키마¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57e1b778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user_id',\n",
       " 'ArrayVoice',\n",
       " 'attend',\n",
       " 'resultPass',\n",
       " 'imageUrl',\n",
       " 'addText',\n",
       " 'name',\n",
       " 'ArrayPercent',\n",
       " 'ArrayDate',\n",
       " 'title',\n",
       " 'type',\n",
       " 'review_wish',\n",
       " 'ArrayAddResult',\n",
       " 'record_time',\n",
       " 'gender',\n",
       " 'birth_year',\n",
       " 'local_code',\n",
       " 'is_test',\n",
       " 'job',\n",
       " 'language',\n",
       " 'perfect',\n",
       " 'excepted_age_array',\n",
       " 'collection',\n",
       " 'ad_duration',\n",
       " 'ad_duration_end',\n",
       " 'desc_images_array',\n",
       " 'images_array',\n",
       " 'thumbnail_image',\n",
       " 'level',\n",
       " 'ad_link',\n",
       " 'participant_count',\n",
       " 'videoUrl']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_point에서 컬럼 이름 변경\n",
    "df_ad_renamed = df_ad.withColumnRenamed(\"ageArray\", \"excepted_age_array\") \\\n",
    "                           .withColumnRenamed(\"date\", \"ad_duration\") \\\n",
    "                           .withColumnRenamed(\"dateEnd\", \"ad_duration_end\") \\\n",
    "                           .withColumnRenamed(\"descArray\", \"desc_images_array\") \\\n",
    "                           .withColumnRenamed(\"imageArray\", \"images_array\") \\\n",
    "                           .withColumnRenamed(\"imageUrl\", \"thumbnail_image\") \\\n",
    "                           .withColumnRenamed(\"link\", \"ad_link\") \\\n",
    "                           .withColumnRenamed(\"player\", \"participant_count\") \\\n",
    "                           .withColumnRenamed(\"name\", \"ad_name\")\n",
    "\n",
    "# df_ad에서 필요한 컬럼만 선택\n",
    "selected_df_ad = df_ad_renamed.select(\"excepted_age_array\", \"collection\", \"ad_duration\", \"ad_duration_end\",\n",
    "                                           \"desc_images_array\", \"images_array\", \"thumbnail_image\", \"level\",\n",
    "                                           \"ad_link\", \"participant_count\", \"videoUrl\", \"ad_name\")\n",
    "\n",
    "# 조인 수행\n",
    "join_condition_ad = (result_df_users[\"name\"] == selected_df_ad[\"ad_name\"])\n",
    "result_df = result_df_users.join(selected_df_ad, join_condition_ad, \"left_outer\")\n",
    "\n",
    "# 중복 컬럼 제거\n",
    "result_df = result_df.drop(selected_df_ad[\"ad_name\"])\n",
    "\n",
    "# 최종 결과 출력\n",
    "result_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e4d7eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ad_duration_end에 값이 있는 행의 개수: 3787257\n",
      "ad_duration_end에 값이 없는 행의 개수: 2926774\n"
     ]
    }
   ],
   "source": [
    "# print(f\"전체 행의 개수: {result_df_point.count()}\")\n",
    "\n",
    "# ad_duration_end 값이 있는 행의 개수\n",
    "ad_duration_end_present_count = result_df.filter(result_df.ad_duration_end.isNotNull()).count()\n",
    "\n",
    "# ad_duration_end 값이 없는 행의 개수\n",
    "ad_duration_end_absent_count = result_df.filter(result_df.ad_duration_end.isNull()).count()\n",
    "\n",
    "# # ArrayDate 값이 없으면서 record_time에 값이 있는 행의 개수\n",
    "# array_date_absent_record_time_present_count = result_df_users.filter(\n",
    "#     (result_df_users.ArrayDate.isNull()) & \n",
    "#     (result_df_users.record_time.isNotNull())\n",
    "# ).count()\n",
    "\n",
    "print(f\"ad_duration_end에 값이 있는 행의 개수: {ad_duration_end_present_count}\")\n",
    "print(f\"ad_duration_end에 값이 없는 행의 개수: {ad_duration_end_absent_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f30f121e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Hadoop\\hadoop-3.3.6\n"
     ]
    }
   ],
   "source": [
    "os.environ['HADOOP_HOME'] = \"C:\\\\Hadoop\\\\hadoop-3.3.6\"\n",
    "print(os.environ[\"HADOOP_HOME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fdd83a2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2119.csv.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:354)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:382)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:847)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:341)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:331)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:370)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:192)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:215)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1111)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 23 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mresult_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoalesce\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD:/DATA_PREPROCESS/FIRESTORE_DATAS/voice_metadata_230829\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\my_conda_01\\lib\\site-packages\\pyspark\\sql\\readwriter.py:1799\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[0;32m   1782\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   1783\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1797\u001b[0m     lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[0;32m   1798\u001b[0m )\n\u001b[1;32m-> 1799\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\my_conda_01\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\my_conda_01\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\my_conda_01\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o2119.csv.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:354)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:382)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:847)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:341)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:331)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:370)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:192)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:215)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1111)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 23 more\r\n"
     ]
    }
   ],
   "source": [
    "result_df.coalesce(10).write.csv(\"D:/DATA_PREPROCESS/FIRESTORE_DATAS/voice_metadata_230829\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769db39a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
